{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Import Libraries"
      ],
      "metadata": {
        "id": "lujkXVvvd7qX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YhBGFDtldQ-n"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import os, pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_notebook_environment():\n",
        "    \"\"\"Returns the environment in which the notebook is running.\"\"\"\n",
        "    if 'KAGGLE_URL_BASE' in os.environ:\n",
        "        return 'Kaggle'\n",
        "    if 'GOOGLE_CLOUD_PROJECT' in os.environ:\n",
        "        return 'GCC'\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "        return 'Google Colab'\n",
        "    return 'local'\n",
        "\n",
        "environment = get_notebook_environment()\n",
        "print(f'You are running on: {environment}')\n",
        "\n",
        "# Define the folder where the dataset is saved\n",
        "folder=''\n",
        "if environment == 'Kaggle':\n",
        "    folder ='/kaggle/input/dataset-bioautex/'\n",
        "elif environment == 'GCC':\n",
        "    folder ='dataset-bioautex/'\n",
        "elif environment == 'Google Colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    folder='/content/drive/My Drive/Biomedicina/dataset-bioautex/'\n",
        "\n",
        "try:\n",
        "    os.chdir(folder)\n",
        "except:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTwAQEFsdV2z",
        "outputId": "04d6687d-8268-4395-8584-fd4d7edf72a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are running on: Google Colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mayority Voting Ensemble"
      ],
      "metadata": {
        "id": "1pZ247qhd-01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def majority_voting(predictions):\n",
        "    \"\"\"\n",
        "    Perform majority voting on the predictions from multiple models.\n",
        "\n",
        "    Args:\n",
        "    predictions (np.array): Array of shape (num_models, num_samples, num_classes) containing predictions from multiple models.\n",
        "\n",
        "    Returns:\n",
        "    np.array: Array of shape (num_samples,) containing the final predictions.\n",
        "    \"\"\"\n",
        "    majority_votes = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "    return majority_votes"
      ],
      "metadata": {
        "id": "RYtKli1zdbaP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define paths to results"
      ],
      "metadata": {
        "id": "Ms193ZLteKEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the desired path to the results folders\n",
        "preds_path = [\n",
        "    \"../ensembler/results/bert-base-multilingual-cased\",\n",
        "    \"../ensembler/results/gpt2\",\n",
        "    \"../ensembler/results/xlm-roberta-base\",\n",
        "    \"../ensembler/results/fast_detect_gpt\",\n",
        "    \"../ensembler/results/mdeberta-v3-base\",\n",
        "    \"../ensembler/results/medical_mT5/\",\n",
        "]\n",
        "\n",
        "# Define the experiments to run\n",
        "experiments = [1,2,3,4,5]\n",
        "\n",
        "# Name of the file to test\n",
        "pred_file_name = 'results_no_dataleak.csv'\n",
        "\n",
        "all_models = True\n",
        "\n",
        "SAVE = True\n",
        "if SAVE:\n",
        "    # create directories to save predictions and scores\n",
        "    os.makedirs('../ensembler/predictions/', exist_ok=True)\n",
        "    os.makedirs('../ensembler/scores/', exist_ok=True)\n",
        "    os.makedirs('../ensembler/matrix/', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "5DG4UrzldgiK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create saving results functions"
      ],
      "metadata": {
        "id": "sqxToktzfPzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ensemble_predictions(ensembler_pred, ensemble_file_name):\n",
        "    \"\"\"Saves the ensembled predictions to a CSV file.\"\"\"\n",
        "    df_ensemble = pd.DataFrame(ensembler_pred, columns=['prediction'])\n",
        "    file_predictions = f'../ensembler/predictions/{ensemble_file_name}.csv'\n",
        "    df_ensemble.to_csv(file_predictions, index=False)\n",
        "    print(f'{file_predictions} saved')\n",
        "\n",
        "def save_ensemble_scores(y_true, ensembler_pred, ensemble_file_name):\n",
        "    \"\"\"Calculates and saves the classification report to a text file.\"\"\"\n",
        "    score_en = classification_report(y_true, ensembler_pred)\n",
        "    print(score_en)\n",
        "    file_scores = f'../ensembler/scores/{ensemble_file_name}.txt'\n",
        "    with open(file_scores, 'w') as f:\n",
        "        f.write(score_en)\n",
        "    print(f'{file_scores} saved')\n",
        "\n",
        "def save_confusion_matrix(y_true, ensembler_pred, ensemble_file_name):\n",
        "    \"\"\"Calculates and saves the confusion matrix to a CSV file.\"\"\"\n",
        "    cm = confusion_matrix(y_true, ensembler_pred)\n",
        "    df_cm = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
        "    file_matrix = f'../ensembler/matrix/{ensemble_file_name}_matrix.csv'\n",
        "    df_cm.to_csv(file_matrix)\n",
        "    print(f'{file_matrix} saved')"
      ],
      "metadata": {
        "id": "ksH4f_rhe8sd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the ensambler"
      ],
      "metadata": {
        "id": "VCUThIzjfTBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for experiment in experiments:\n",
        "    model_preds = []\n",
        "    y_true_loaded = None\n",
        "\n",
        "    for path in preds_path:\n",
        "        file_path = os.path.join(path, str(experiment), pred_file_name)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        predictions = df['Prediction'].values\n",
        "        model_preds.append(predictions)\n",
        "\n",
        "        if y_true_loaded is None:\n",
        "            y_true_loaded = df['label'].values\n",
        "\n",
        "    all_predictions = np.array(model_preds)\n",
        "    ensembler_pred = majority_voting(all_predictions)\n",
        "\n",
        "    ensemble_file_name = f\"MV_ens_all_models_exp_{experiment}\" if all_models else f\"MV_ens_no_mT5_{experiment}\"\n",
        "\n",
        "    if SAVE:\n",
        "        save_ensemble_predictions(ensembler_pred, ensemble_file_name)\n",
        "        save_ensemble_scores(y_true_loaded, ensembler_pred, ensemble_file_name)\n",
        "        save_confusion_matrix(y_true_loaded, ensembler_pred, ensemble_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJXesIK7dwDP",
        "outputId": "c498e814-ceff-452a-d239-3beb69530006"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../ensembler/predictions/MV_ens_all_models_exp_1.csv saved\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.68      0.77      1250\n",
            "           1       0.74      0.91      0.82      1250\n",
            "\n",
            "    accuracy                           0.80      2500\n",
            "   macro avg       0.81      0.80      0.79      2500\n",
            "weighted avg       0.81      0.80      0.79      2500\n",
            "\n",
            "../ensembler/scores/MV_ens_all_models_exp_1.txt saved\n",
            "../ensembler/matrix/MV_ens_all_models_exp_1_matrix.csv saved\n",
            "../ensembler/predictions/MV_ens_all_models_exp_2.csv saved\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.64      0.74      1250\n",
            "           1       0.72      0.91      0.80      1250\n",
            "\n",
            "    accuracy                           0.77      2500\n",
            "   macro avg       0.80      0.77      0.77      2500\n",
            "weighted avg       0.80      0.77      0.77      2500\n",
            "\n",
            "../ensembler/scores/MV_ens_all_models_exp_2.txt saved\n",
            "../ensembler/matrix/MV_ens_all_models_exp_2_matrix.csv saved\n",
            "../ensembler/predictions/MV_ens_all_models_exp_3.csv saved\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.75      0.79      1250\n",
            "           1       0.77      0.85      0.81      1250\n",
            "\n",
            "    accuracy                           0.80      2500\n",
            "   macro avg       0.81      0.80      0.80      2500\n",
            "weighted avg       0.81      0.80      0.80      2500\n",
            "\n",
            "../ensembler/scores/MV_ens_all_models_exp_3.txt saved\n",
            "../ensembler/matrix/MV_ens_all_models_exp_3_matrix.csv saved\n",
            "../ensembler/predictions/MV_ens_all_models_exp_4.csv saved\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.60      0.72      1250\n",
            "           1       0.70      0.92      0.80      1250\n",
            "\n",
            "    accuracy                           0.76      2500\n",
            "   macro avg       0.79      0.76      0.76      2500\n",
            "weighted avg       0.79      0.76      0.76      2500\n",
            "\n",
            "../ensembler/scores/MV_ens_all_models_exp_4.txt saved\n",
            "../ensembler/matrix/MV_ens_all_models_exp_4_matrix.csv saved\n",
            "../ensembler/predictions/MV_ens_all_models_exp_5.csv saved\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.63      0.73      1250\n",
            "           1       0.71      0.91      0.80      1250\n",
            "\n",
            "    accuracy                           0.77      2500\n",
            "   macro avg       0.79      0.77      0.77      2500\n",
            "weighted avg       0.79      0.77      0.77      2500\n",
            "\n",
            "../ensembler/scores/MV_ens_all_models_exp_5.txt saved\n",
            "../ensembler/matrix/MV_ens_all_models_exp_5_matrix.csv saved\n"
          ]
        }
      ]
    }
  ]
}